---
title: "Biostats Week 7: Bivariate for one categorical and one continuous variable"
output: html_document
urlcolor: blue
---

# Outline

This packet reviews the basics of _hypothesis testing_ and several specific hypothesis tests to compare continuous variables across groups:

#. Review null hypothesis significance testing (NHIST)
#. One-sample t-test
#. Independent samples t-test
#. Dependent or paired t-test
#. Analysis of Variance (ANOVA)
#. Testing assumptions
#. Optional Items

#1. Null hypothesis significance testing (NHIST)

Relationships among variables are often described as being *statistically significant* or *not statistically significant*. This terminology comes primarily from the process of NHIST. NHIST starts with writing a null hypothesis which states that there is NO RELATIONSHIP or NO EFFECT between variables.  

The process of NHIST is: 

#. Write the null and alternate hypotheses 
#. Compute the appropriate test statistic 
#. Calculate the probability that your test statistic is as big as it is if there is no relationship (i.e., the null is true) 
#. If the probability that the null is true is very small--less than 5%--reject the null hypothesis (if the p is low, the null must go!)
#. If the probability that the null is true is not small--5% or higher--retain the null hypothesis 

#2. One-sample t-test 

The _one-sample t-test_ compares a sample mean to a hypothesized or population mean. For example, the dotted line represents in the graphic represents an actual observed mean and the red is the hypothesized or population mean. The one-sample t-test compares the actual mean to the hypothesized mean to determine if the sample likely comes from a population with the hypothesized mean.

```{r distributionImage1, echo=FALSE}
library(ggplot2)

# for right now, this is just making a toy dataset to work with
dat = data.frame(d = rnorm(n = 100, mean = 25, sd = 5))

ggplot(data = dat, aes(x = d)) +
  geom_density(fill="lightblue") +
  geom_vline(xintercept = 20, color = "red") +
  geom_vline(xintercept = 25, linetype = "dashed") +
  labs(title = "What Is The Question?", subtitle = "one-sample t-test",
       x = "", y = "")
```

There is information on the number of cigarettes smoked per day by smokers in the NHANES data set. One pack of cigarettes holds 20 cigarettes and many smokers smoke 1 pack per day. To test NHANES smokers against this hypothetical mean, we can compare the mean number of cigarettes smoked per day during the past 30 days by the NHANES participants to the hypothetical value of 20. 

The research question might be: Do NHANES smokers smoke a mean of 20 cigarettes (1 pack) per day?

The one-sample t-test uses the t-statistic (sort of like a z-statistic) as the test statistic: 

\[
t=\frac{m-\mu}{\frac{s}{\sqrt{n}}}
\]

*  $m$ is the sample mean
*  $\mu$ is the population mean (or hypothetical mean)
*  $s$ is the sample standard deviation
*  $n$ is the sample size

```{r dataPrepForOneSampleT, message=FALSE}
# Bring in the NHANES data
library(RNHANES)
smokeNHANES <- nhanes_load_data(file_name = "SMQ_H",
                               year = "2013-2014",
                               demographics = TRUE)

# clean the number of cigarettes variable
library(car)

#### NOTE: if using recode gives you an error, add the package
#### name with :: before the function, like this - car::recode

smokeNHANES$numcigs <- car::recode(smokeNHANES$SMD650, "777 = NA; 999 = NA")

# mean and standard deviation for number of cigarettes
meanCigs <- mean(smokeNHANES$numcigs, na.rm = TRUE)
sdCigs <- sd(smokeNHANES$numcigs, na.rm = TRUE)

# library with function to get sample size treating NA as missing
library(naniar)

# get sample size
nSubjects = n_complete(smokeNHANES$numcigs)
```

NHIST Step 1: Write the null and alternate hypotheses 

Null (H0): Smokers smoke an average of 20 cigarettes per day  
Alternate (HA): Smokers do not smoke an average of 20 cigarettes per day

Start with a graph to make a guess about what you think you are going to find.
```{r plotHistOneSampleT, warning=FALSE, message=FALSE}
# explore the data with a graph
library(ggplot2)
ggplot(data = smokeNHANES, aes(x = numcigs)) +
  geom_density(fill = 'pink') + 
  ggtitle("Number of cigarettes smoked by \n smokers each day (NHANES 2013-2014)")
```


NHIST Step 2: Compute the appropriate test statistic 

$$t=\frac{`r meanCigs` - 20}{\frac{`r sdCigs`}{\sqrt{`r nSubjects`}}}=
`r ((meanCigs - 20)/(sdCigs/sqrt(nSubjects)))`$$

```{r oneSampleT}
# conduct the t-test with the hypothesized 
# population mean (mu) as 20
t.test(smokeNHANES$numcigs, mu=20)
```

NHIST Step 3:  Calculate the probability that your test statistic is as big as it is if there is no relationship (i.e., if the null is true)

The t-statistic is -40.181 and the p-value for the t-statistic is < 2.2e-16 or < .00000000000000022, which is lower than the standard cutoff of .05. Visually, the _critical region_ or _rejection region_ of the t-distribution is outside the dotted lines:

```{r tDist, echo=FALSE}
# t-distribution 
x <- seq(-4, 4, length=100)
y <- dnorm(x)
plot(x, y, type="l", main="t-distribution", xlab = "t-statistic", ylab = "Probability density")
abline(v=1.96, col="orange", lty="dashed")
abline(v=-1.96, col="orange", lty="dashed")
```

Another way of saying this:

The probability of getting a t-statistic of -40.181 or even more extreme, if the null hypothesis is true, is < .00000000000000022. That is a *very* low probability of getting such a large t-statistic, so the null is *probably not true*. We should *reject the null hypothesis*.

**If p is low, the null must go.**

NHIST Step 4 & 5: If the probability that the null is true is very small--less than 5%--reject the null hypothesis (if the p is low, the null must go!); if the probability that the null is true is not small--5% or higher--retain the null hypothesis.

There is sufficient evidence to reject the null hypothesis. The mean of 10.41 cigarettes smoked per day is statistically significantly different from the hypothesized population mean of 20 cigarettes per day (t = -40.18; p < .05). The sample of smokers likely came from a population with a daily mean smoking rate of between 9.94 and 10.87 cigarettes per day (see confidence intervals from the t.test printout).

#### A Note About Tails...

If you have clear predictions (based on theory in your field) on the *direction* of your effect, use a one-tailed test. If you do *not* know the direction of your effect, use a two-tailed test.

Notice the difference in hypotheses:

-  **one-tailed test:**
    * H0: sample mean = hypothesized/population mean
    * HA: sample mean > hypothesized/population mean
-  **two-tailed test:**
    * H0: sample mean = hypothesized/population mean
    * HA: sample mean $\neq$ hypothesized/population mean

Most of the time, we are using two-tailed tests because we aren't sure about the direction.

# 3. Independent samples t-test

Instead of comparing one mean to a hypothesized or population mean, the independent samples t-test compares the means of two groups to each other to see if they likely come from populations with the same mean. In the NHANES smoker data there are males and females. Some research shows women smokers start at younger ages on average. We can use the independent samples t-test to find out if this is the case. 

NHIST Step 1:

H0: There is no difference in mean age of starting smoking for male and female smokers 

HA: There is a difference in mean age of starting smoking for male and female smokers

Start with plotting the two groups:
```{r indepTPlot, warning=FALSE, message=FALSE}
# recode the variables
smokeNHANES$sex <- car::recode(smokeNHANES$RIAGENDR, "1 = 'Male'; 2 = 'Female'")
smokeNHANES$age.smoking <- car::recode(smokeNHANES$SMD030, "0 = NA; 
                              777 = NA; 999 = NA")

# compare male and female smoking age with density plot
ggplot(data = smokeNHANES, aes(fill = sex, x = age.smoking)) + 
  geom_density(alpha=.5) + 
  xlab("Age started smoking (in years)") + 
  ylab("Probability density") +
  ggtitle("Age started smoking (NHANES, 2013-2014)")

# compare male and female smoking age with boxplot
ggplot(data = smokeNHANES, aes(x = sex, y = age.smoking)) + 
  geom_boxplot() + 
  xlab("Participant sex") + 
  ylab("Age started smoking (in years)") +
  ggtitle("Age started smoking (NHANES, 2013-2014)")
```

Check out some distributions with means shown:

```{r indepTDistributions, echo=FALSE, fig.width=8, fig.height=8}
library(gridExtra)

# let's make another toy dataset of scores
set.seed(500)

group1 <- data.frame(scores = rnorm(n = 100, mean = 0, sd = 1),
                    group = rep("Group 1", times = 100))
group2a <- data.frame(scores = rnorm(n = 100, mean = .5, sd = 1),
                     group = rep("Group 2", times = 100))

# store the means of each group to use for plotting later
m1 <- mean(group1$scores)
m2a <- mean(group2a$scores)

dat2 = rbind(group1, group2a)

p1 <- ggplot(data = dat2, aes(x = scores)) +
  geom_density(aes(fill = group), alpha = .5) +
  geom_vline(xintercept = m1, color = "red") +
  geom_vline(xintercept = m2a, color = "blue") +
  coord_cartesian(xlim = c(-4,4), ylim = c(0,.5)) +
  labs(x = "scores")


# Now, let's keep our group 1, but what if group 2 had a much larger mean?
group2b <- data.frame(scores = rnorm(n = 100, mean = 1.25, sd = 1),
                     group = rep("Group 2", times = 100))
m2b <- mean(group2b$scores)

dat3 <- rbind(group1, group2b)

p2 <- ggplot(data = dat3, aes(x = scores)) +
  geom_density(aes(fill = group), alpha = .5) +
  geom_vline(xintercept = m1, color = "red") +
  geom_vline(xintercept = m2b, color = "blue") +
  coord_cartesian(xlim = c(-4,4), ylim = c(0,.5)) +
  labs(x = "scores")

# and again...

group2c <- data.frame(scores = rnorm(n = 100, mean = 2.25, sd = 1),
                     group = rep("Group 2", times = 100))
m2c <- mean(group2c$scores)

dat4 <- rbind(group1, group2c)

p3 <- ggplot(data = dat4, aes(x = scores)) +
  geom_density(aes(fill = group), alpha = .5) +
  geom_vline(xintercept = m1, color = "red") +
  geom_vline(xintercept = m2c, color = "blue") +
  coord_cartesian(xlim = c(-4,4), ylim = c(0,.5)) +
  labs(x = "scores")

layoutGrid = rbind(c(1,2), c(3,NA))
grid.arrange(p1, p2, p3, ncol=2, layout_matrix=layoutGrid,
             top = "What Is The Question?")

```



NHIST Step 2:

\[
t=\frac{m_1-m_2}{\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}}
\]

Get means, standard deviations, and sample sizes for male and female:
```{r dataPrepIndepT}
# use tableone package with 
# strata option for groups
library(tableone)
smoking.age.sex <- CreateTableOne(vars = "age.smoking", 
                                  strata = "sex", 
                                  data = smokeNHANES)
smoking.age.sex

```

Plug n chug:
\[
t=\frac{18.66-17.48}{\sqrt{\frac{6.06^2}{3714}+\frac{4.59^2}{3454}}}=5.25
\]

Or, write one line of code:
```{r indepT}
# t-test for age smoking initiation by sex
t.test(smokeNHANES$age.smoking ~ smokeNHANES$sex)
```

NHIST Step 3: 
The p-value for the t-statistic is 1.705e-07 or .0000001705, which is lower than the standard cutoff of .05. Visually, the _critical region_ or _rejection region_ of the t-distribution is outside the dotted lines that are boundaries around the middle 95% of data:

```{r tDist2, echo=FALSE}
# t-distribution
x <- seq(-4, 4, length=100)
y <- dnorm(x)
plot(x, y, type="l", main="t-distribution", xlab = "t-statistic", ylab = "probability density")
abline(v = 1.96, col="orange", lty="dashed")
abline(v = -1.96, col="orange", lty="dashed")
```

NHIST Step 4 & 5: 

There is sufficient evidence to reject the null hypothesis (t = 5.25; p < .05). The mean age of smoking initiation for males (m = 17.48) is statistically significantly different from the mean age of smoking initiation for females (m = 18.66). Male smokers and female smokers in this sample likely come from populations with different mean ages of smoking initiation. Males start smoking more than a year earlier than females.

####Bonus!

Let's say you've read a lot of studies on when people start smoking, and you notice that in every study, males start smoking before females. You want to see if this is true in your data. How can you modify this `t.test(smokeNHANES$age.smoking ~ smokeNHANES$sex)` code to test your hypothesis? (Hint: `?t.test` or `help(t.test)`). Make a new code chunk by clicking on the Insert button at the top of this pane, and try it for yourself!

#4. Dependent samples t-test

Sometimes the means you want to compare will be related (not independent groups). For example, the mean number of cigarettes smoked or the mean BMI before and after an intervention. When the two groups being compared are related (same people before & after, siblings, spouses, or two otherwise matched groups) an adjustment to the t-test to account for the non-independence is used. Everything else about the test stays the same! See the Dalgaard text for more information.

#5. Analysis of Variance (ANOVA)

When you have three or more group means to compare you need something a little stronger than a t-test. The statistical test for comparing means across 3 or more groups is ANOVA. For example, we could test the mean age of smoking initiation by race/hispanic origin. 

NHIST Step 1:

H0: There is no difference in the mean age of smoking initation across race/ethnicity groups.  

HA: There is a difference in the means. 

Start with exploring the groups using descriptive statistics and a graph:

```{r anovaBoxPlot, warning=FALSE, message=FALSE}
# recode the race-eth variable
smokeNHANES$race.eth <- car::recode(smokeNHANES$RIDRETH1, "1 = 'Mexican-Amer';
2 = 'Other Hispanic'; 3 = 'White Non-Hisp'; 4 = 'Black Non-Hisp';
5 = 'Other Race'")

# this is a factor variable or a category, so tell R to treat it as such!
smokeNHANES$race.eth <- factor(smokeNHANES$race.eth)

# table of the means
smoking.age.race <- CreateTableOne(vars = "age.smoking",
                                   strata = "race.eth", 
                                   data = smokeNHANES)
smoking.age.race

# plot race-eth by groups 
ggplot(data = smokeNHANES, aes(x = race.eth, y = age.smoking)) + 
  geom_boxplot(aes(fill = race.eth)) + 
  xlab("Participant race") + 
  ylab("Age started smoking (in years)") +
  ggtitle("Age started smoking (NHANES, 2013-2014)") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) 
```

NHIST Step 2:

```{r anova}
# use aov for ANOVA
race.smoking.age <- aov(age.smoking ~ race.eth, data = smokeNHANES)
summary(race.smoking.age)
```

NHIST Steps 3-5: 

There is evidence that the means are statistically significantly different from one another (F(4, 2443) = 10.78; p < .05). The five groups likely have come from populations with different mean age of smoking initiation. 

ANOVA is an _omnibus test_ so even though the graph and descriptive statistics show which group means seem different from one another, we need a statistical test to determine which means are *statistically significantly* different from one another. There are several different tests to choose from. The Bonferroni post-hoc test is one possibility: 

```{r anovaPostHoc}
# post-hoc test to determine which means are different
pairwise.t.test(smokeNHANES$age.smoking, smokeNHANES$race.eth)
```

The values in the small table are the p-values for t-tests of each pair of means. It appears that the mean age of smoking initiation for Non-Hispanic Blacks (m = 18.98) is significantly (p = .03) lower than for Mexican-Americans (m = 17.81). The age of initiation for Non-Hispanic Whites (m = 17.38) was statistically significantly (p < .05) lower than the age of initiation for Non-Hispanic Blacks (m = 18.98) and Other Race (m = 18.90) participants.

#6. Testing assumptions

All statistical tests make some underlying assumptions about the data being tested. Just like the mean is not a great indicator of central tendency when the distribution is skewed, statistical tests like the t-tests and ANOVA are not great when the data fail to meet the underlying assumptions. 

The one-sample t-test, independent samples t-test, and ANOVA rely on a few main assumptions: 

* Independence of observations: Each observation in the data set is unrelated to the others in the data set. If your data includes siblings, spouses, or other related observations, it may not meet this assumption. *See Dependent Samples t-test*
* Normal distribution: The outcome variable is normally distributed  
* Homogeneity of variance: The groups have the same or similar variance for the outcome (not applicable in the one-sample test because there is only one group)

### How to test each assumption 

_Independence of observations_ is not tested but instead is known based on how the data were collected. 

_Normal distribution_ is checked visually using a histogram. It can also be tested statistically using the Shapiro-Wilk test. _The Shapiro-Wilk test tests the null hypothesis that the data are normally distributed_. 

First, check for the one-sample t-test:

```{r shapiroWilk1}
# check the distribution of numcigs
hist(smokeNHANES$numcigs)

# check normality statistically with the Shapiro-Wilk test
shapiro.test(smokeNHANES$numcigs)
```

Doesn't look normally distributed by the plot. For the S-W test, given the p-value less than .05, we reject the null hypothesis that numcigs is normally distributed and conclude that numcigs is not normally distributed. This assumption is NOT MET.

Try the age.onset for the independent samples t-test and ANOVA:

```{r shapiroWilk2}
# check the distribution of age.smoking
hist(smokeNHANES$age.smoking) 

# check normality statistically with the Shapiro-Wilk test
shapiro.test(smokeNHANES$age.smoking)
```

Doesn't look normally distributed by either the plot or the S-W test! Given the p-value for the S-W test is less than .05, we reject the null hypothesis that age.smoking is normally distributed and conclude that age.smoking is not normally distributed. This assumption is NOT MET.

_Homogeneity of variance_ is checked using the Levene Test. The Levene Test tests the null hypothesis that the variances are equal across groups. To meet this assumption we DO NOT want to reject the null hypothesis. Since this is about equal variance across groups, it is not relevant for the one-sample t-test because that test has only one group. 

```{r levene1, message=FALSE, warning=FALSE}
#Check the HoV of age.smoking from the independent samples t-test
leveneTest(age.smoking ~ sex, data = smokeNHANES)
```

The small p-value indicates we *rejected* the null hypothesis that the variances are equal, so we do not meet this assumption. This assumption is NOT MET.

```{r, levene2, warning=FALSE, message=FALSE}
#Check the HoV from the ANOVA
leveneTest(age.smoking ~ race.eth, data = smokeNHANES)
```

The small p-value indicates we did reject the null, which means we do not meet this assumption. This assumption is NOT MET.

So, none of the tests met their assumptions! When data _do not_ meet the assumptions for a specific test there are 3 options: 

#. Use a different test (the non-parametric version)  
#. Transform the continuous variable to meet the normality assumption and try again 
#. Report your results as not meeting assumptions and therefore not generalizable beyond the sample

## Alternative to the one-sample t-test

The non-parametric version of the one-sample t-test is the Sign Test. The sign test computes whether the median is statistically significantly different than some population or hypothesized median. So, instead of comparing the sample mean to the mu, the Sign Test compares the sample median to a population or hypothesized median, like this:

H0: The median number of cigarettes smoked per day by NHANES smokers is 20. 

HA: The median number of cigarettes smoked per day by NHANES smokers is not 20.

```{r, signTest, message=FALSE}
# conduct the sign with the hypothesized 
# population median as 20 
library(BSDA)
SIGN.test(smokeNHANES$numcigs, md=20)
```

The s statistic is 58 and the p-value for the s-statistic is < 2.2e-16 or < .00000000000000022, which is lower than the standard cutoff of .05. 

The Sign Test indicates that the NHANES participants do not come from a population that smokes a median of 20 cigarettes per day of 20 (s = 58; p < .05). The median in the sample is 10 with a 95% confidence interval of 9 to 10 cigarettes per day. The sample likely comes from a population of smokers that smokes a median of 9 to 10 cigarettes per day.

## Alternative to the independent samples t-test 

The non-parametric version of the independent samples t-test is the Mann-Whitney U test (also called the Wilcoxon test). Although sometimes interpreted as the difference in medians, the Mann-Whitney U test actually puts all the observations from both groups in order from lowest and highest and ranks them. The sums of the ranks for observations in the two groups are then compared. The null and alternate hypothesis would be:

H0: Age of smoking initiation is equally distributed for males and females

HA: Age of smoking initiation is not equally distributed for males and females

Revisiting the distributions from earlier:

```{r mannWhitPlot, warning=FALSE, message=FALSE}
# compare male and female smoking age
ggplot(data = smokeNHANES, aes(fill = sex, x = age.smoking)) + 
  geom_density(alpha=.5) + 
  xlab("Participant sex") + 
  ylab("Age started smoking (in years)") +
  ggtitle("Age started smoking (NHANES, 2013-2014)")
```

Conduct the test:

```{r mannWhit} 
# testing for differences in distributions 
wilcox.test(age.smoking ~ sex, data = smokeNHANES)

```

In this case, it appears to be significant. Since this is a non-parametric test for data that are not normally distributed, compute medians (instead of means) to add some context:

```{r mannWhit2}
# table of medians and IQR
smoking.age.sex <- CreateTableOne(vars = "age.smoking", 
                                  strata = "sex", 
                                  data = smokeNHANES)
print(smoking.age.sex, nonnormal = "age.smoking")
```

We can conclude that there is a statistically significant difference (W = 797420; p = .0002) in the distribution of age of initation for males and females. Males likely start smoking at an earlier age (med = 17 years) than females (med = 18 years).

## Alternative for ANOVA 

Likewise, the non-parametric version of ANOVA is the _Kruskal-Wallis test_, which examines ranks across groups like the Mann-Whitney U test, but for more than two groups. The two arguments for this test are x, which is the continuous variable and g, which is the groups. The command can fail if the groups variable is not a factor data type, so you can add as.factor to ensure it will run. 

Try the Kruskal-Wallis test for age of smoking initiation by the race.eth variable:

H0: The distribution of the age of smoking initiation is the same by race/ethnicity.

HA: The distribution of the age of smoking initiation differs by race/ethnicity.

```{r kwTest}
# examine distributions across groups 
kruskal.test(x = smokeNHANES$age.smoking, g = as.factor(smokeNHANES$race.eth))

```

In this case, it appears to be significant. So, we can conclude that there is a statistically significant difference (K-W chi-squared = 61.59; p < .05) in the distribution of age of smoking initiation by race/ethnicity. A table can add some additional context: 

```{r kwTest2}
# table of medians and IQR for age of smoking initiation
# by race
smoking.age.race <- CreateTableOne(vars = "age.smoking", 
                                  strata = "race.eth", 
                                  data = smokeNHANES)
print(smoking.age.race, nonnormal = "age.smoking")
```

It appears that Mexican-Americans, Non-Hispanic white, and Other Hispanic all have median age of smoking initiation of 17 years old while Non-Hispanic Black and Other Race participants have a median of 18 years old for smoking initiation. 

Like the post-hoc test for the t-test, to determine which of these groups are statistically significant from one another, a Dunn's test can be used to compare each pair of groups:

```{r dunnTest, message=FALSE}
# open the dunn.test package
library(dunn.test)
dunn.test(x = smokeNHANES$age.smoking, g = smokeNHANES$race.eth)
```

The Dunn's post-hoc test indicates that the age of smoking initiation distribution is significantly (p < .05) different for Non-Hispanic Blacks (med = 18) compared to Mexican-Americans (med = 17), Other Hispanics (med = 17), and Non-Hispanic Whites (med = 17). Other Race participants (med = 18) are also statistically significantly different (p < .05) from Other Hispanic and Non-Hispanic Whites for age of smoking initiation. Non-Hispanic Blacks and Other Race participants start smoking leter (med = 18) than the other groups (med = 17). 

# Optional Items

##WARNING!

**The following is intended for those who are comfortable in R, and feel reasonably comfortable with the material covered in this document**

**If you do *not* feel comfortable with R or the material presented here, do not worry about anything below**

####Accessing Your Models

Previously in this document, we we ran a model but we did not store it as a new object (so it printed out right away). Often, you'll want to store these models, and access only certain parts. Here's how you can do that:

```{r access1}
indepTTest <- t.test(smokeNHANES$age.smoking ~ smokeNHANES$sex)
```

Notice that our new object `indepTTest` is stored in your Global Environment as a list object. You can access different parts the same way you would access a column in a dataframe `$`.

```{r access2}
indepTTest$p.value
```

Sometimes, there are multiple items, but you only want one of them. Here, you can use indexing. 

```{r access3}
# both confidence intervals
indepTTest$conf.int

# lower confidence interval
indepTTest$conf.int[1]

# upper confidence interval
indepTTest$conf.int[2]
```

This is nice because it means we can store things for later use.

Lastly, sometimes you'll see multiple items, but those items are also named. But you don't want the names! You want to store just the value or the number. We can still use indexing, but we'll use double brackets `[[]]`.

Think about a book. `[]` gives you the title of the book, sometimes the table of contents, and if it's short enough, even the whole book. `[[]]` actually opens the book. So you don't see the title anymore, but you can easily go through page by page. 

```{r access4}
# Compare:
indepTTest$estimate

indepTTest$estimate[1]
indepTTest$estimate[[1]]

indepTTest$estimate[2]
indepTTest$estimate[[2]]
```

You do not have to use the `$`. You can also use an index. However, this sometimes changes the bracket structure. When trying this at home, if you see errors, it's likely that you're using single brackets when you should use double brackets.

```{r access5}
# lower confidence interval

# this...
indepTTest$conf.int[1]
# is the same as this:
indepTTest[4][[1]][1]

# let's go through why:
indepTTest[4] # gives CIs, but it's named as $conf.int

indepTTest[4][1]# gives same as above bc the whole thing is the 1st element

indepTTest[4][[1]]# now it takes away the named part, $conf.int

indepTTest[4][[1]][1]# first element is lower.ci!

```


####Helpful Visualizations:

*  [One-Sample Tests](https://shiny.rit.albany.edu/stat/betaprob/). These principles will apply to independent samples, too!

*  [Effect Sizes](http://rpsychologist.com/d3/cohend/). This is a blog that has a ton of great visualizations. I'm linking the one for effect sizes, but there's also a great one on NHST and confidence intervals. 

*  [ANOVA](https://students.brown.edu/seeing-theory/regression-analysis/index.html#section3). Great visualization of ANOVAs, as well as a bunch of others like basic probability theory, the central limit theorem, and others.

####Using `dplyr` for getting things:

`dplyr` is a package that feeds very nicely into `ggplot2` and other packages. It's also a nice way of summarizing data in a way that you can easily access later.

```{r dplyr1, warning=FALSE, message=FALSE}
library(dplyr)

# Let's get means, standard deviations, and medians of age.smoking, 
# but let's summaize it by race/ethnicity

# Note, make sure that the thing you want to summarize by is actually a 
# factor! You can check that with the `class()` function

class(smokeNHANES$race.eth)

summaryStats <- smokeNHANES %>%
  group_by(race.eth) %>% 
  summarize(meanAge = mean(age.smoking, na.rm = TRUE),
            sdAge = sd(age.smoking, na.rm = TRUE),
            medianAge = median(age.smoking, na.rm = TRUE)) 

```

This is particularly useful if you ever want to make error bars in your ggplot!

```{r dplyr2}
ggplot(data = summaryStats, aes(x = race.eth, y = meanAge)) +
  geom_col(position = "dodge", aes(fill = race.eth)) +
  geom_errorbar(aes(ymin = meanAge - sdAge, ymax = meanAge + sdAge),
                position = "dodge",
                width = .2) +
  theme(axis.text.x = element_text(angle=45, hjust=1))

```

